# Source Code Classification and Similarity Analysis
### Author: [Vladimir Zolotov](mailto:zolotov@us.ibm.com)

## Introduction

This repository holds an experimental package for classification and similarity analysis of source code.

The goal of this project is to investigate capabilities of simple neural network architectures and deep learning techniques for source code classification and similarity analysis.

All applications are written in python 3 and uses Keras deep learning tool with Tensorflow backend.

The current version of the code operates with two types of source code representations:
* bag of tokens;
* sequence of tokens.

## Data processing flow
The package can process input data from two types of sources:
* IBM CodeNet dataset with its metadata files.
* Two level directory structure like POJ-104 dataset. Top level directory has a set of sub-directories corresponding to problems solved with programs represented with source code files. Each sub-directory has source code of programs solving the corresponding problem.

The source code is processed in 2 stages:
1. Tokenizing source code solutions of the selected problems satisfying specified criteria ( problems having only sufficient number of solutions in a specified language, etc) and writing them down into the local database.
2. Classification or similarity analysis of all or some part of the tokenized source code files stored in the local database.

Having a local database with tokenized source code classification and similarity analysis can be performed multiple times for different subsets of source code files.

## Package applications

Current version of the package has the following applications:
*  *MakeTokenizedDS.py* tokenizes source code solutions of the selected problems stored in CodeNet, and writes them down into local database.
* *TokenizeImportDS.py* tokenize source code solutions stored in a dataset different from CodeNet in format similar to POJ-104.
* *BagOfTokensClassifier.py* trains DNN for classifying source code using bag of tokens technique.
* *SimilarityByBoT.py* trains DNN for similarity analysis of source code using bag of tokens technique.
* *ClasBagTokEval.py* evaluates a trained bag of tokens classifier and performs confusion analysis.
* *SimBagTokEval.py* evaluates a trained bag of tokens similarity analyzer and performs confusion analysis.
* *SeqOfTokensClassifier.py* trains DNN for classifying source code using sequence of tokens technique. The DNN is trained using either CPU or single GPU.
* *SimilarityBySeqTok.py* trains DNN for similarity analysis of source code using sequence of tokens technique. The DNN is trained using either CPU or single GPU.
* *SeqClassParallel.py* trains DNN for classifying source code using sequence of tokens technique. The DNN is trained using multiple GPUs in data parallel mode.
* *SimSeqTokParallel.py* trains DNN for similarity analysis of source code using sequence of tokens technique. The DNN is trained using multiple GPUs in data parallel mode.
* *ClasSeqTokEvalParall.py* evaluates a trained sequence of tokens classifier and performs confusion analysis. It runs in multi-GPU mode.
* *SimSeqTokEvalParall.py* evaluates a trained sequence of tokens similarity analyzer and performs confusion analysis.  It runs in multi-GPU mode.
* *SimSeqTokFullTest.py*  evaluates a trained sequence of tokens similarity analyzer on full test including all possible similar and dissimilar pairs of source code files.
* *MapAtR.py* computes Map@R accuracy metric  of a trained sequence of tokens similarity analyzer.  It runs in multi-GPU mode.
* *ClassDsVerify.py* verifies consistency of train, validation and test datasets split generated by bag of tokens or sequence of tokens classifiers.
* *SimDsVerify.py* verifies consistency of  of train, validation and test datasets generated by bag of tokens or sequence of tokens similarity analyzers.

Applications *MakeProblemSet.py*,  *MakeTokenizedDS.py*, and  *TokenizeImportDS.py* are stored in directory *DSMaker*.

Applications *BagOfTokensClassifier.py*, *SimilarityByBoT.py*, *ClasBagTokEval.py* and *SimBagTokEval.py* are stored in directory *BagOfTokens*.

Applications *SeqOfTokensClassifier.py*, *SimilarityBySeqTok.py*, *SeqClassParallel.py*, *SimSeqTokParallel.py* are stored in directory *SeqOfTokens*.

Applications *ClasSeqTokEvalParall.py*, *SimSeqTokEvalParall.py*, *SimSeqTokFullTest.py*, and *MapAtR.py* are stored in directory *PostProcessor*.

Applications *ClassDsVerify.py* and *SimDsVerify.py* are stored in directory *Verify*.

## Package organization. 

Current version of the package is stored in the following directories:
* *src* holds the source code of the package.
* *run* holds the scripts for running the package applications on CodeNet benchmark datasets.

### Source code of package

The package source code is stored in the directory *src*. It has the following sub-directories:

* *DSMaker* holds applications and their modules for getting, tokenizing and storing in a local database tokenized representation of source code files for classification and similarity analysis.
* *BagOfTokens* holds applications and their modules for classifications and similarity analysis of source code using bag of tokens technique.
* *SeqOfTokens* holds applications and their modules for classifications and similarity analysis of source code using sequence of tokens technique.
* *PostProcessor* holds applications for confusion analysis of classification and similarity analysis results.
* *Verify* holds applications for verifying consistency of training, validation and test datasets generated by the classification or similarity analysis applications.
* *Dataset* holds modules for constructing datasets for training, validation and testing DNNs.
* *ModelMaker* holds modules for constructing DNNs.
* *CommonFunctions* hold utility modules used by several package applications.
* *Clustering* holds experimental modules for cluster analysis of source code.

### Scripts to run package applications

The scripts to run the package applications for source code classification and similarity analysis on the CodeNet benchmarks are stored in the directory *run*.

The directory *run* has the sub-directories corresponding to CodeNet benchmark datasets: *cpp1000*, *cpp1400*, *java250*, *python800*. Current version has only scripts for cpp1000 benchmark dataset.

Each of the above directories has the following sub-directories:
* *tokenize* holds script *tokenize.sh*, which runs the package application for tokenization of the corresponding CodeNet benchmark dataset.
* *class* holds sub-directories *bagtok* and *seqtok* with scripts for training and testing source classifiers using bag-of-tokens and sequence-of-tokens techniques respectively. 

Both *bagtok* and *seqtok* directories has same organization and hold similar scripts:
* *train.sh* for training the corresponding source code classifier.
* *eval.sh* for testing and evaluation of the trained source code classifier.
* *verify.sh* for checking consistency of the dataset split into training, validation and testing portions. This split is constructed for training and testing of other source code classifiers based on the techniques different from bag-of-tokens or sequence-of-tokens.

## Running scripts on CodeNet benchmark datasets

Before training and evaluation source code classifiers or similarity analyzers, it is required to tokenize the benchmark dataset. 

Here we explain how to run tokenization, training and evaluation scripts for specific case of *cpp1000* benchmark for source code classification using bag-of-tokens technique. The scripts for all other benchmarks, both for source code classification and similarity analysis, using either bag-of-tokens or sequence-of-tokens technique are ran in the similar way. The corresponding scripts has same names and same external parameters. However, they call different package applications with different set of parameters.

### Tokenization

For tokenizing *cpp1000* dataset go to *run/cpp1000/tokenize/* directory and run *tokenize.sh* script specifying a path to the directory with the *cpp1000* benchmark dataset as follows:

`./tokenize.sh <Path to cpp1000 benchmark dataset>`

The script creates the directory *cpp1000ds* with a database of the tokenized source code files of the problem solutions. This database is used training and evaluation scripts both for classification and similarity analysis, both for the bag-of-tokens and the sequence-of-tokens techniques. Therefore, it is enough to run *tokenize.sh* script only once for all further runs of training and evaluation scripts on *cpp1000* benchmark.

The script also creates the directory *TokenizerWorkDir* holding listing files with tokenization reports.

### Traning and testing neural networks

After completing tokenization of the benchmark dataset either any source code classifier or any similarity analyzer can be trained by running *train.sh* script. For training bag-of-tokens source code classifier go to the directory */run/cpp1000/class/bagtok/* and run *train.sh* script, specifying the desired number of training epochs, as follows:

`./train.sh <Number of training epochs>`

The script creates the following directories:
* *class_ckpt* holds check points with trained classifier model. These models are used with the script evaluating them on test dataset.
* *dataset_statistics* holds files with datasets statistics and training, validation and test datasets for other classifiers.

The trained models are evaluated with *eval.sh* script on the test dataset. The script has no external parameters, as all application parameters a set inside it. By default the script evaluates the best trained model form *class_ckpt* directory.  The script updates *dataset_statistics* directory with reports on the test dataset, and creates confusion_report directory with the classification confusion report and other files reporting the classification statistics.

The training and testing scripts generate specification of datasets used for traning and testing the neural networks. Those specifications are in the form of lists of samples referring the corresponding source code files. The consistency of the generated dataset specifications against the benchmark dataset can be checked with *verify_ds.sh* script. The script can be ran as follows:

`/verify_ds.sh <Path to cpp1000 benchmark dataset>`

The script prints some statistics on the datasets and error messages if any inconsistencies are found.

## Benchmark results

The results of running the package applications on the CodeNet benchmark results are shown in the following table 1-4.

It should be noted that due to variations of GPU computation results for large neural networks on long training sequences, the best accuracy can vary from one run to another one, as much as up to ~1%. Besides the number of epoch at which the best accuracy is achieved also varies.

| Benchmark dataset | Accuracy | # tokens      |
| ------------ | ----------- | ------------------- | 
| Java250    | 71.87   | 81     |
| Python800    | 67.74 | 71     |
| C++1000     | 68.23  | 56     |
| C++1400     | 64.73  | 56     |

Table 1: Classification results using MLP with bag of tokens

| Benchmark dataset | Accuracy | # tokens      |
| :------------ | :-----------: | -------------------: |
| Java250    | 90.96   | 81     |
| Python800    | 89.49 | 71     |
| C++1000     | 94.27  | 56     |
| C++1400     | 94.10  | 56     |

Table 2: Classification results using CNN with sequence of tokens

| Benchmark dataset | Accuracy | # tokens      | # of test samples |
| :------------ | :-----------: | -------------------: | -------------------: |
| Java250    | 82.48   | 81     | 512000 |
| Python800    | 86.64 | 71     | 512000 |
| C++1000     | 85.69  | 56     | 512000 |
| C++1400     | 86.47  | 56     | 512000 |

Table 3: Similarity results using MLP with bag of tokens

| Benchmark dataset | Accuracy | # tokens      | # of test samples |
| :------------ | :-----------: | -------------------: | -------------------: |
| Java250    | 89.39   | 75     | 512000 |
| Python800    | 94.86 | 71     | 512000 |
| C++1000     | 96.70  | 56     | 512000 |
| C++1400     | 96.36  | 56     | 512000 |

Table 4: Similarity results using Siamese network with sequence of tokens

